{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "YLcAu3mXJOpL",
    "outputId": "257224dd-3225-4e02-903e-95ae1671b033"
   },
   "outputs": [],
   "source": [
    "### PREREQUISTIES\n",
    "### (many used only in one notebook...)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging ### to monitor the code\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import csv\n",
    "import unicodedata\n",
    "import requests\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import gspread\n",
    "from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
    "#from google.colab import auth\n",
    "#from oauth2client.client import GoogleCredentials\n",
    "from google.oauth2 import service_account # based on google-auth library\n",
    "import sddk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim parts\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "\n",
    "### lsa alternative\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "W2TxLXhwp_zD",
    "outputId": "a39f89e7-c18c-4fc1-aae8-6cf288fef849"
   },
   "outputs": [],
   "source": [
    "#!pip install anda\n",
    "#from anda import gr ### the import takes substantial time, since it import a +600MB file containing ancient Greek dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "Vk65k519TvBW",
    "outputId": "c0784c15-d327-49c5-9807-f0c957b4aef5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sciencedata.dk username (format '123456@au.dk'): 648597@au.dk\n",
      "sciencedata.dk password: ········\n",
      "endpoint variable has been configured to: https://sciencedata.dk/files/\n"
     ]
    }
   ],
   "source": [
    "### not neccessary for reading the data, just for exporting them to sciencedata.dk\n",
    "conf = sddk.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access gsheet, you need Google Service Account key json file\n",
    "# I have mine located in my personal space on sciencedata.dk, so I read it from there:\n",
    "\n",
    "# (1) read the file and parse its content\n",
    "file_data = conf[0].get(conf[1] + \"ServiceAccountsKey.json\").json()\n",
    "# (2) transform the content into crendentials object\n",
    "credentials = service_account.Credentials.from_service_account_info(file_data)\n",
    "# (3) specify your usage of the credentials\n",
    "scoped_credentials = credentials.with_scopes(['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive'])\n",
    "# (4) use the constrained credentials for authentication of gspread package\n",
    "gc = gspread.Client(auth=scoped_credentials)\n",
    "# (5) establish connection with spreadsheets specified by their url\n",
    "PIA_data = gc.open_by_url(\"https://docs.google.com/spreadsheets/d/1KxOx7Be9fj3lDcEPgQhQ-Iqcn9p367-MMD6RMXe8rks/edit?usp=sharing\")\n",
    "PIA_overview = gc.open_by_url(\"https://docs.google.com/spreadsheets/d/1e94wyelg6dftQ4zxbq1xvwxWAI-BhcYXtclDW-YTnrw/edit?usp=sharing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file located in a public folder\n"
     ]
    }
   ],
   "source": [
    "publicfolder = \"31b393e2afe1ee96ce81869c7efe18cb\"\n",
    "c_hippocraticum = sddk.read_file(\"c_hippocraticum_enriched.json\", \"df\", publicfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove = [\"ἢν\", \"ὁκόταν\"]\n",
    "\n",
    "c_hippocraticum[\"lemmatized_sentences_repl\"] = c_hippocraticum[\"lemmatized_sentences_repl\"].apply(lambda sentences_list:  [[word for word in sentence if word not in words_to_remove] for sentence in sentences_list])\n",
    "\n",
    "words_to_replace = {\n",
    "    \"ἔχις\" : \"ἔχω\"}\n",
    "\n",
    "for key in words_to_replace.keys():\n",
    "    c_hippocraticum[\"lemmatized_sentences_repl\"] = c_hippocraticum[\"lemmatized_sentences_repl\"].apply(lambda sentences_list: [[corrections[key] if x == key else x for x in sentence] for sentence in sentences_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and preprocess corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['filename', 'author', 'title', 'string', 'wordcount', 'author_id',\n",
       "       'doc_id', 'raw_date', 'date_avr', 'date_probs', 'date_manual',\n",
       "       'provenience', 'sentences', 'lemmata', 'lemmata_wordcount',\n",
       "       'lemmatized_sentences', 'n_sentences', 'lemmata_repl',\n",
       "       'lemmatized_sentences_repl', 'λύπ*', 'ἄλγ*', 'ὀδύν*', 'πόνο*',\n",
       "       'terms_sum', 'λύπ*_TF', 'ἄλγ*_TF', 'ὀδύν*_TF', 'πόνο*_TF', 'TF_sum',\n",
       "       'work_cat_jouanna', 'work_cat_craik', 'work_cat_linka'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_hippocraticum.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flat_sentences(series):\n",
    "    sentences_list = [sent for doc in series.tolist() for sent in doc]\n",
    "    return sentences_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = sentences\n",
    "docs = get_flat_sentences(c_hippocraticum[\"lemmatized_sentences_repl\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ὁπόσος', 'ἐπιχειρέω', 'ἰητρικῆς', 'λέγω', 'γράφω', 'ὑπόθεσις', 'λόγος', 'θερμός', 'ψυχρός', 'ὑγρός', 'ξηρός', 'ἐθέλω', 'βραχὺ', 'ἀρχην', 'αἰτία', 'ἄνθρωπος', 'νόσος', 'θάνατος', 'πᾶς', 'ἵημι', 'πολύς', 'λέγω', 'καταφανής', 'εἶμι', 'ἄξιος', 'μέμφομαι', 'τέχνη', 'χρέονταί', 'πᾶς', 'μέγας', 'τιμάω', 'ἀγαθοὺς', 'χειροτέχνης', 'δημιουργός'], ['εἶμι', 'δημιουργός', 'φαῦλος', 'πολύς'], ['ἰητρικη', 'ὅλοξ', 'μηδʼ', 'ἔσκεπτο', 'μηδʼ', 'εὑρίσκω', 'μηδείς', 'πᾶς', 'ὅμοιος', 'ἄπειροί', 'ἀνεπιστήμων', 'τύχη', 'πᾶς', 'κάμνω', 'διοικεῖτο'], ['ἔχω', 'τέχνη', 'πᾶς', 'δημιουργός', 'πολύς', 'ἀλλήλων', 'διαφέρω', 'χείρ', 'γνώμη', 'ἰητρικῆς'], ['ἀξιόω', 'αὐτην', 'κενός', 'ὑπόθεσις', 'δέω', 'ἀφανέα', 'ἀπορεόμενα', 'ἀνάγκη', 'ἐπιχειρέω', 'λέγω', 'ὑπόθεσις', 'χράομαι', 'οἷος', 'μετέωρος', 'γῆ'], ['εἶμι', 'λέγω', 'γιγνώσκω', 'ἔχω', 'οὔτʼ', 'λέγω', 'ἀκούω', 'δῆλος', 'ἐάω', 'ἀληθής'], ['χρῆ', 'ἀνενέγκαντα', 'οἶδα', 'σαφής'], ['ἰατρικός', 'πάλη', 'πᾶς', 'ὑπάρχω', 'ἀρχη', 'ὁδός', 'ἣν', 'πολύς', 'καλός', 'εὑρίσκω', 'πολύς', 'χρόνος', 'λοιπός', 'εὑρίσκω', 'ἱκανός', 'ἐὼν', 'εἰδὼς', 'ζητέω'], ['ἀποβαλὼν', 'ἀποδοκιμάζω', 'πᾶς', 'ἕτερος', 'ὁδός', 'ἕτερος', 'σχῆμα', 'ἐπιχειρέω', 'ζητέω', 'φημί', 'ἐξευρίσκω', 'ἐξαπατάω', 'ἐξαπατάω'], ['ἀδύνατος']]\n"
     ]
    }
   ],
   "source": [
    "print(docs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171233"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_hippocraticum[\"lemmata_repl\"].apply(lambda x: len(x)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical 2504\n",
      "Other 16911\n",
      "Practical 5044\n"
     ]
    }
   ],
   "source": [
    "# perhaps we will also explore our subcorpora at some point\n",
    "subcorpora = {}\n",
    "for cat in c_hippocraticum[\"work_cat_linka\"].unique():\n",
    "    subcorpora[cat] = get_flat_sentences(c_hippocraticum[c_hippocraticum[\"work_cat_linka\"]==cat][\"lemmatized_sentences_repl\"])\n",
    "\n",
    "for key in subcorpora.keys():\n",
    "    print(key, len(subcorpora[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create gensim dictionary for our list of sentences\n",
    "dictionary = corpora.Dictionary(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is a number of methods to be applied upon a dictionary object:\n",
    "see: https://radimrehurek.com/gensim/corpora/dictionary.html\n",
    "\n",
    "First of all, you can inspect it as a standard dictionary object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below to print the whole dictionary\n",
    "# dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'αἰτία',\n",
       " 1: 'βραχὺ',\n",
       " 2: 'γράφω',\n",
       " 3: 'δημιουργός',\n",
       " 4: 'εἶμι',\n",
       " 5: 'θάνατος',\n",
       " 6: 'θερμός',\n",
       " 7: 'καταφανής',\n",
       " 8: 'λέγω',\n",
       " 9: 'λόγος'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as such, it is organized by ids\n",
    "dict(list(dictionary.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'αἰτία': 0,\n",
       " 'βραχὺ': 1,\n",
       " 'γράφω': 2,\n",
       " 'δημιουργός': 3,\n",
       " 'εἶμι': 4,\n",
       " 'θάνατος': 5,\n",
       " 'θερμός': 6,\n",
       " 'καταφανής': 7,\n",
       " 'λέγω': 8,\n",
       " 'λόγος': 9}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but you can access it reveresly by applying token2id method\n",
    "dict(list(dictionary.token2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collection_frequencies\n",
    "dictionary.cfs[5] # how many instances of word with id 5 (= \"θάνατος\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.dfs[5] # how many documents contain the word with id 5 (= \"θάνατος\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167358"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.num_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25059"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['αὐτη',\n",
       " 'ἀνάγκη',\n",
       " 'ἰητρικην',\n",
       " 'ποιέω',\n",
       " 'ζητηθῆναί',\n",
       " 'εὑρίσκω',\n",
       " 'ἄνθρωπος',\n",
       " 'κάμνω',\n",
       " 'ταὐτάζω',\n",
       " 'προσφερομένοισι',\n",
       " 'συμφέρω',\n",
       " 'συμφέρω']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = docs[20]\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(23, 1),\n",
       " (34, 1),\n",
       " (35, 1),\n",
       " (57, 1),\n",
       " (127, 1),\n",
       " (128, 2),\n",
       " (131, 1),\n",
       " (132, 1),\n",
       " (133, 1),\n",
       " (134, 1),\n",
       " (135, 1)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doc2bow = document to (term, tf) tuples, i.e. bag-of-words\n",
    "dictionary.doc2bow(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA with  sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use our gensim dictionary\n",
    "\n",
    "# as a corpus, we cannot use the Gensim default BoW model,\n",
    "\n",
    "# we just need the words replaced by values\n",
    "\n",
    "corpus_bow = [dictionary.doc2bow(sent) for sent in sentences_list]\n",
    "corpus_idx = [dictionary.doc2idx(sent) for sent in sentences_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24459"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec =  TfidfVectorizer(vocabulary=vocabulary) ### initiaze the model\n",
    "X = vec.fit_transform([\" \".join(sentence) for sentence in list_of_lists]) ### run the model\n",
    "Xc = (X.T * X)\n",
    "svd = TruncatedSVD(n_components=25, n_iter=5, random_state=42)\n",
    "svd.fit(Xc)\n",
    "cooc = pd.DataFrame(Xc.toarray(), columns=vec.get_feature_names(), index=vec.get_feature_names())\n",
    "lsa_model_data = pd.DataFrame(svd.components_, columns=vec.get_feature_names())\n",
    "\n",
    "\n",
    "def get_most_similar(model_df, target_term, number):\n",
    "  all_similar = []\n",
    "  for term in model_df.columns:\n",
    "    similarity = (term, cosine_similarity([model_df[target_term],  model_df[term]])[0][1])\n",
    "    all_similar.append(similarity)\n",
    "  return sorted(all_similar, key=lambda number: number[1], reverse=True)[1:number]\n",
    "\n",
    "model_john_lsa, cooc_john = lsa_model(sentences_john, words_john)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsamodel, cooc = lsa_model( vocabulary=)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPzJG8Ttu74+AUFBxNNmOYH",
   "include_colab_link": true,
   "name": "OVERVIEW.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
